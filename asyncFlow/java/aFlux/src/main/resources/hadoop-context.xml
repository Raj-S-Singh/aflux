<?xml version="1.0" encoding="UTF-8"?>
<!--
  ~ /*
  ~  * IoT BigData Middleware : Interfacing IoT Mashups and Big Data
  ~  * Copyright (C) 2017  Software & Systems Engineering , TUM
  ~  *
  ~  * This program is free software; you can redistribute it and/or
  ~  * modify it under the terms of the GNU General Public License
  ~  * as published by the Free Software Foundation; either version 2
  ~  * of the License, or (at your option) any later version.
  ~  *
  ~  * This program is distributed in the hope that it will be useful,
  ~  * but WITHOUT ANY WARRANTY; without even the implied warranty of
  ~  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  ~  * GNU General Public License for more details.
  ~  *
  ~  * You should have received a copy of the GNU General Public License
  ~  * along with this program; if not, write to the Free Software
  ~  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
  ~  */
  ~
  -->

<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:hdp="http://www.springframework.org/schema/hadoop"
       xmlns:beans="http://www.springframework.org/schema/beans"
       	xmlns:p="http://www.springframework.org/schema/p"
       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
		http://www.springframework.org/schema/hadoop http://www.springframework.org/schema/hadoop/spring-hadoop.xsd ">

    <!-- Hadoop Configuration -->
    
    <!-- 
    
    client running this configuration should have access to the ip port of datanodes as they are declared on namenodes and datanodes
    
    In case where datanodes are running on docker or vm should be made changes on network to have access to the address namenode gives to the client
    with instructions like
    sudo ifconfig lo0 alias 172.17.0.2
    (Making forward 172.17.0.2 (in this case) to loacalhost for the case 172.17.0.2:50010 is forwarded to 127.0.0.1:50010 for example)
    
    
     -->
	<!--use the bean definition to go beyond the configuration options provided by the namespace -->
	<bean name="ppc" class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer" 
		p:valueSeparator="|"  p:systemPropertiesModeName="SYSTEM_PROPERTIES_MODE_OVERRIDE" p:order="100">
    <property name="locations">
       <list>
           <value>classpath:application.properties</value>
       </list>
    </property>
		</bean>


    <hdp:configuration id="hadoopConfiguration" properties-location="classpath:application.properties"
    	file-system-uri="hdfs://${hadoop.host}:${hadoop.fs.port}"
    
    >
        fs.defaultFS=hdfs://${hadoop.host}:${hadoop.fs.port}
        dfs.client.block.write.replace-datanode-on-failure.enable=true
         hadoop.tmp.dir=file://${java.io.tmpdir}
        <!-- 

        mapred.job.tracker=${hadoop.host}:${hadoop.mapred.port}
        mapreduce.framework.name=yarn
        yarn.resourcemanager.address=${hadoop.host}:8050
        yarn.log.server.url=${hadoop.host}:19888
        yarn.resourcemanager.resource-tracker.address=${hadoop.host}:8025
        zookeeper.znode.parent=/hbase-unsecure
        
        
        
        
         <property>
			  <name>dfs.datanode.address</name>
  				<value>127.0.0.1:50010</value>
  			<description>
    			The datanode server address and port for data transfer.
  			</description>
		</property>
         
        
         -->
         
         
    </hdp:configuration>

	<bean id="fsShell" class="org.springframework.data.hadoop.fs.FsShell">
		<constructor-arg name="configuration" ref="hadoopConfiguration"/> 
		<constructor-arg name="fs" ref="hadoopFs"/>
	</bean>
	
	<bean id="hadoopFs" class="org.springframework.data.hadoop.fs.FileSystemFactoryBean" p:configuration-ref="hadoopConfiguration"/>	
    <!-- Pig Configuration -->
    <hdp:pig-factory properties-location="classpath:application.properties" exec-type="MAPREDUCE" job-name="pig-script" configuration-ref="hadoopConfiguration"
                     xmlns="http://www.springframework.org/schema/hadoop"/>
    <hdp:pig-template/>


    <hdp:pig-runner id="pigRunner" run-at-startup="false">
       <hdp:script>
           A = load '/user/mahapatr/passwd/input/passwd' using PigStorage(':');
           B = foreach A generate $0 as id;
           dump B;
       </hdp:script>
   </hdp:pig-runner>
    <!-- Map Reduce Configurations -->
    <!-- 
    <hdp:job id="wordcountJob"
         input-path="${wordcount.input.dir}"
         output-path="${wordcount.output.dir}"
         mapper="de.tum.in.jobs.WordCount$TokenizerMapper"
         reducer="de.tum.in.jobs.WordCount$IntSumReducer"
         user="root" 
          />

    <hdp:job-runner id="runner" job-ref ="wordcountJob" run-at-startup="false"/>
 -->
    <!-- Pig Configuration -->
    
    
    <!-- 
    <hdp:pig-factory properties-location="classpath:application.properties" exec-type="MAPREDUCE" job-name="pig-script" configuration-ref="hadoopConfiguration"
                     xmlns="http://www.springframework.org/schema/hadoop"/>

    <hdp:pig-template/>

    <hdp:pig-runner id="pigRunner" run-at-startup="false">
       <hdp:script>
           A = load '/user/mahapatr/passwd/input/passwd' using PigStorage(':');
           B = foreach A generate $0 as id;
           dump B;
       </hdp:script>
   </hdp:pig-runner>
 -->
    <!-- Hive Configuration -->
    
    <beans:bean id="hiveDriver" class="org.apache.hive.jdbc.HiveDriver"/>
    <beans:bean id="hiveDataSource" class="org.springframework.jdbc.datasource.SimpleDriverDataSource" >
        <beans:constructor-arg index="0" ref="hiveDriver"/>
        <beans:constructor-arg index="1" value="jdbc:hive2://${hive.host}:${hive.port}/${hive.db}"/>
        <beans:constructor-arg index="2" value="${hive.user}"/>
        <beans:constructor-arg index="3" value=""/>
    </beans:bean>
    <bean class="org.springframework.jdbc.core.JdbcTemplate" id="hiveTemplate">
        <constructor-arg ref="hiveDataSource"/>
    </bean>
    
    
    
</beans>