<?xml version="1.0" encoding="UTF-8"?>


<!--
  ~ aFlux: JVM based IoT Mashup Tool
  ~ Copyright 2019 Tanmaya Mahapatra
  ~
  ~ Licensed under the Apache License, Version 2.0 (the "License");
  ~ you may not use this file except in compliance with the License.
  ~ You may obtain a copy of the License at
  ~
  ~     http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:hdp="http://www.springframework.org/schema/hadoop"
       xmlns:beans="http://www.springframework.org/schema/beans"
       	xmlns:p="http://www.springframework.org/schema/p"
       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
		http://www.springframework.org/schema/hadoop http://www.springframework.org/schema/hadoop/spring-hadoop.xsd ">

    <!-- Hadoop Configuration -->
    
    <!-- 
    
    client running this configuration should have access to the ip port of datanodes as they are declared on namenodes and datanodes
    
    In case where datanodes are running on docker or vm should be made changes on network to have access to the address namenode gives to the client
    with instructions like
    sudo ifconfig lo0 alias 172.17.0.2
    (Making forward 172.17.0.2 (in this case) to loacalhost for the case 172.17.0.2:50010 is forwarded to 127.0.0.1:50010 for example)
    
    
     -->
	<!--use the bean definition to go beyond the configuration options provided by the namespace -->
	<bean name="ppc" class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer" 
		p:valueSeparator="|"  p:systemPropertiesModeName="SYSTEM_PROPERTIES_MODE_OVERRIDE" p:order="100">
    <property name="locations">
       <list>
           <value>classpath:application.properties</value>
       </list>
    </property>
		</bean>


    <hdp:configuration id="hadoopConfiguration" properties-location="classpath:application.properties"
    	file-system-uri="hdfs://${hadoop.host}:${hadoop.fs.port}"
    
    >
        fs.defaultFS=hdfs://${hadoop.host}:${hadoop.fs.port}
        dfs.client.block.write.replace-datanode-on-failure.enable=true
         hadoop.tmp.dir=file://${java.io.tmpdir}
        <!-- 

        mapred.job.tracker=${hadoop.host}:${hadoop.mapred.port}
        mapreduce.framework.name=yarn
        yarn.resourcemanager.address=${hadoop.host}:8050
        yarn.log.server.url=${hadoop.host}:19888
        yarn.resourcemanager.resource-tracker.address=${hadoop.host}:8025
        zookeeper.znode.parent=/hbase-unsecure
        
        
        
        
         <property>
			  <name>dfs.datanode.address</name>
  				<value>127.0.0.1:50010</value>
  			<description>
    			The datanode server address and port for data transfer.
  			</description>
		</property>
         
        
         -->
         
         
    </hdp:configuration>

	<bean id="fsShell" class="org.springframework.data.hadoop.fs.FsShell">
		<constructor-arg name="configuration" ref="hadoopConfiguration"/> 
		<constructor-arg name="fs" ref="hadoopFs"/>
	</bean>
	
	<bean id="hadoopFs" class="org.springframework.data.hadoop.fs.FileSystemFactoryBean" p:configuration-ref="hadoopConfiguration"/>	
    <!-- Pig Configuration -->
    <hdp:pig-factory properties-location="classpath:application.properties" exec-type="MAPREDUCE" job-name="pig-script" configuration-ref="hadoopConfiguration"
                     xmlns="http://www.springframework.org/schema/hadoop"/>
    <hdp:pig-template/>


    <hdp:pig-runner id="pigRunner" run-at-startup="false">
       <hdp:script>
           A = load '/user/mahapatr/passwd/input/passwd' using PigStorage(':');
           B = foreach A generate $0 as id;
           dump B;
       </hdp:script>
   </hdp:pig-runner>
    <!-- Map Reduce Configurations -->
    <!-- 
    <hdp:job id="wordcountJob"
         input-path="${wordcount.input.dir}"
         output-path="${wordcount.output.dir}"
         mapper="de.tum.in.jobs.WordCount$TokenizerMapper"
         reducer="de.tum.in.jobs.WordCount$IntSumReducer"
         user="root" 
          />

    <hdp:job-runner id="runner" job-ref ="wordcountJob" run-at-startup="false"/>
 -->
    <!-- Pig Configuration -->
    
    
    <!-- 
    <hdp:pig-factory properties-location="classpath:application.properties" exec-type="MAPREDUCE" job-name="pig-script" configuration-ref="hadoopConfiguration"
                     xmlns="http://www.springframework.org/schema/hadoop"/>

    <hdp:pig-template/>

    <hdp:pig-runner id="pigRunner" run-at-startup="false">
       <hdp:script>
           A = load '/user/mahapatr/passwd/input/passwd' using PigStorage(':');
           B = foreach A generate $0 as id;
           dump B;
       </hdp:script>
   </hdp:pig-runner>
 -->
    <!-- Hive Configuration -->
    
    <beans:bean id="hiveDriver" class="org.apache.hive.jdbc.HiveDriver"/>
    <beans:bean id="hiveDataSource" class="org.springframework.jdbc.datasource.SimpleDriverDataSource" >
        <beans:constructor-arg index="0" ref="hiveDriver"/>
        <beans:constructor-arg index="1" value="jdbc:hive2://${hive.host}:${hive.port}/${hive.db}"/>
        <beans:constructor-arg index="2" value="${hive.user}"/>
        <beans:constructor-arg index="3" value=""/>
    </beans:bean>
    <bean class="org.springframework.jdbc.core.JdbcTemplate" id="hiveTemplate">
        <constructor-arg ref="hiveDataSource"/>
    </bean>
    
    
    
</beans>